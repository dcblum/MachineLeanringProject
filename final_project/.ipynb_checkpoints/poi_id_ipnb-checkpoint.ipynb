{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Project Fraud Classifer\n",
    "\n",
    "### Goal: Use Machine Learning to Assess Fraud from Financial and Email Data \n",
    "\n",
    "Many companies keep datasets of thier financial and email data. These datasets are very large; it is extremely difficult to cipher through all of the data. Having a machine assist and find a potential, fraudulent person of interest (POI) would save both time and money.\n",
    "\n",
    "The email data set tested is the Enron Corpus, which can be found at (https://www.cs.cmu.edu/~./enron/). Enron was one of the top energy companies in the US in the early 2000s. The company eventually filed Bankruptcy largely due to fraudulent cases of insider trading and accounting scandals. It is one of the most noteworthy cases of fraud in the 20th century. Because of the scale of the company and the size of the email database, the Enron Corpus is a prime dataset for finding clues of fraud via financials and email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data and Find Important Features\n",
    "\n",
    "Initial exploration of the dataset revealed a couple outliers. Most entries were of people's names: 'SKILLING JEFFREY K' or 'LAY KENNETH L'; however both 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK' were listed as if they were actual people that worked for Enron. Because both were not living beings, I decided to remove them from calculations. \n",
    "\n",
    "Features were hand picked from visual aide via https://public.tableau.com/profile/diego2420#!/vizhome/Udacity/UdacityDashboard. Further inspection of data could be made with already built-in-functions to find the best fit features.\n",
    "\n",
    "Important Features picked for determining POI:\n",
    "* exercised_stock_options (high values ~ POI)\n",
    "* deferred_income (low values ~ POI)\n",
    "* expenses (low values ~ not POI)\n",
    "\n",
    "Three additional features were calculated but were found to be of minimal importance: percent_to_poi, percent_from_poi, and email_poi_score. The thought was to assign a score to each person signifying the capacity they were in contact with a POI. The score was calculated by summing a person's percent of emails _to_ or _from_ a POI. Again, this yielded no significant gain and was discarded as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "all_features = ['poi', 'salary', 'deferral_payments', 'total_payments',\n",
    "'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income',\n",
    "'total_stock_value', 'expenses', 'exercised_stock_options', 'other',\n",
    "'long_term_incentive', 'restricted_stock', 'director_fees', 'to_messages',\n",
    "'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi',\n",
    "'shared_receipt_with_poi']\n",
    "\n",
    "current_max_features_list = [\"poi\", \"exercised_stock_options\",\n",
    "\"deferred_income\", \"expenses\"]\n",
    "\n",
    "features_list = [\"poi\", \"exercised_stock_options\", \"deferred_income\",\n",
    " \"expenses\"]\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "del data_dict['TOTAL']\n",
    "del data_dict['THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             salary         bonus\n",
      "count  9.500000e+01  8.200000e+01\n",
      "mean   5.621943e+05  2.374235e+06\n",
      "std    2.716369e+06  1.071333e+07\n",
      "min    4.770000e+02  7.000000e+04\n",
      "25%    2.118160e+05  4.312500e+05\n",
      "50%    2.599960e+05  7.693750e+05\n",
      "75%    3.121170e+05  1.200000e+06\n",
      "max    2.670423e+07  9.734362e+07\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "###creating dataFrame from dictionary - pandas\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index', dtype=np.float)\n",
    "print df.describe().loc[:,['salary','bonus']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "\n",
    "# Create feature: email_poi_score\n",
    "# email_poi_score is the sum of normalized message data for from_poi and to_poi\n",
    "def normalize_feature(feature, data_dict):\n",
    "    # initialize high and low value for normalization function\n",
    "    value_high = None\n",
    "    value_low = None\n",
    "\n",
    "    # loop through persons to find high and low values for features\n",
    "    for person in data_dict:\n",
    "        value = data_dict[person][feature]\n",
    "        if value != 'NaN':\n",
    "            # If first value in feature then assign value to variables\n",
    "            if value_low == None:\n",
    "                value_high = value\n",
    "                value_low = value\n",
    "            # look to see if value is higher or lower\n",
    "            if value > value_high:\n",
    "                value_high = value\n",
    "            elif value < value_low:\n",
    "                value_low = value\n",
    "\n",
    "    # loop to assign normalization value\n",
    "    for person in data_dict:\n",
    "        value = float(data_dict[person][feature])\n",
    "        # if value exists between high and low\n",
    "        if (value_high >= value) and (value_low <= value):\n",
    "            # if denominator isn't zero\n",
    "            if value_high != value_low:\n",
    "                value_norm = (value - value_low) / (value_high - value_low)\n",
    "                data_dict[person][feature] = value_norm\n",
    "\n",
    "\n",
    "\n",
    "# find percent emails sent to poi and percent from poi to this person\n",
    "for person in data_dict:\n",
    "    from_messages = data_dict[person]['from_messages']\n",
    "    to_messages = data_dict[person]['to_messages']\n",
    "    from_poi = data_dict[person]['from_poi_to_this_person']\n",
    "    to_poi = data_dict[person]['from_this_person_to_poi']\n",
    "\n",
    "    data_dict[person]['email_poi_score'] = 'NaN'\n",
    "\n",
    "    percent_to = float(to_poi) / float(from_messages)\n",
    "    percent_from = float(from_poi) / float(to_messages)\n",
    "\n",
    "    data_dict[person]['percent_to_poi'] = percent_to\n",
    "    data_dict[person]['percent_from_poi'] = percent_from\n",
    "\n",
    "# normailize percent_to_poi and percent_from_poi and add together\n",
    "normalize_feature('percent_to_poi', data_dict)\n",
    "normalize_feature('percent_from_poi', data_dict)\n",
    "\n",
    "# add normalized percent_to_poi and percent_from_poi to create email_poi_score\n",
    "for person in data_dict:\n",
    "    percent_to_norm = data_dict[person]['percent_to_poi']\n",
    "    percent_from_norm = data_dict[person]['percent_from_poi']\n",
    "\n",
    "    email_poi_score = percent_to_norm + percent_from_norm\n",
    "    if email_poi_score >= 0:\n",
    "        data_dict[person]['email_poi_score'] = email_poi_score\n",
    "\n",
    "\n",
    "# Normalize features, DON'T normail poi (feature[0] is 'poi')\n",
    "for feature in features_list[1:]:\n",
    "    normalize_feature(feature, data_dict)\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "\n",
    "Data was tested with multiple classifiers consisting of individual trials of **raw** and **normalized** data. I noticed a very high variation when testing; some classifiers yielded high precision and recall scores, but when retested both scores occassionally dropped to zero. To counteract the high deviation of values, I wrote a few functions to generate 1000 classifiers of each type and compare their average scores.\n",
    "\n",
    "Because SVM 'poly' took an extraordinant amount of time and never finished with raw data **only** a **normalized** data classifier was tested for SVM 'poly'\n",
    "\n",
    "Output Data from Classifiers (normalized data only):\n",
    "~~~\n",
    "Note:  STD_sum is the sum of individual standard deviation scores (for simplification)\n",
    "############################################################\n",
    "                    DecisionTree\n",
    "\n",
    "Precision Mean:  0.386631629482\n",
    "Recall Mean:  0.381794300144\n",
    "STD_sum:  0.426737950995\n",
    "############################################################\n",
    "                    RandomForest\n",
    "\n",
    "Precision Mean:  0.381161507937\n",
    "Recall Mean:  0.197606565657\n",
    "STD_sum:  0.505336869258\n",
    "############################################################\n",
    "                    ExtraTree\n",
    "\n",
    "Precision Mean:  0.4745251443\n",
    "Recall Mean:  0.253108910534\n",
    "STD_sum:  0.514709115944\n",
    "############################################################\n",
    "                    SVM 'rbf'\n",
    "\n",
    "Precision Mean:  0.4215\n",
    "Recall Mean:  0.0818132395382\n",
    "STD_sum:  0.580025182133\n",
    "############################################################\n",
    "                    SVM 'poly'\n",
    "\n",
    "Precision Mean:  0.024\n",
    "Recall Mean:  0.00459642857143\n",
    "STD_sum:  0.183943409806\n",
    "############################################################\n",
    "                    GaussianNB\n",
    "\n",
    "Precision Mean:  0.522593650794\n",
    "Recall Mean:  0.258691630592\n",
    "STD_sum:  0.482226413296\n",
    "############################################################\n",
    "~~~\n",
    "\n",
    "The classifier chosen as best is DecisionTree, followed somewhat closely by GaussianNB. DecisionTree maintains higher combined precision and recall scores while having relatively low standard deviations for both scores. Precision is the percent of people the classifier accurately labeled as a POI. Recall is the percent of actual POIs that were correctly classified. Although other classifiers have higher individual precision and recall scores (or individually having lower standard deviation) no other classifiers appear to be a better all-around compared to DecisionTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "                    Decision Tree\n",
      "\n",
      "Precision Mean:  0.476857142857\n",
      "Recall Mean:  0.44\n",
      "STD_sum:  0.738255806836\n",
      "\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from parameter grid:\n",
      "{'max_depth': 5, 'min_samples_split': 2}\n",
      "\n",
      "Complete set of parameters for best estimator:\n",
      "{'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 5,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_split': 1e-07,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'presort': False,\n",
      " 'random_state': 49,\n",
      " 'splitter': 'best'}\n",
      "\n",
      "F1 score for default decision tree:\n",
      "0.533333333333\n",
      "\n",
      "F1 score for best estimator from grid search\n",
      "0.444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'##### Random Forest #####\\n# Does okay...\\nfrom sklearn.ensemble import RandomForestClassifier\\ntest_prec_recall(\"Random Forest\", RandomForestClassifier())\\n\\n##### Extra Trees #####\\n# Current best if using all available features, but still just okay..\\nfrom sklearn.ensemble import ExtraTreesClassifier\\ntest_prec_recall(\"Extra Tree\", ExtraTreesClassifier())\\n\\n##### SVMS #####\\n# So far linear, poly, and rfb SVMs are pretty bad at predicting pre-normalize\\nfrom sklearn.svm import SVC\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\npipe = Pipeline(steps=[(\\'minmaxscaler\\', MinMaxScaler()),(\\'clf\\', SVC())])\\n\\nlabels, features = targetFeatureSplit(data)\\nfeatures_train, features_test, labels_train, labels_test = train_test_split(\\n    features, labels, test_size=0.33)\\n\\npipe.fit(features_train, labels_train)\\n\\ngridCV\\n\\n\\n\\n##### Naive Bayes #####\\n# Naive Bayes never predicts true positive, but can predict true negative.\\nfrom sklearn.naive_bayes import GaussianNB\\ntest_prec_recall(\"Naive Bayes\", GaussianNB())\\n\\n###### Best clf appears to be Decison Tree;\\n###### precision and recall mean > .3\\n###### generally lowest sum of precision and recall standard deviations\\nclf = tree.DecisionTreeClassifier()\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def test_prec_recall(name, clf_choice):\n",
    "    precision_list = list()\n",
    "    recall_list = list()\n",
    "    f1_list = list()\n",
    "    for i in range(100):\n",
    "        ### Extract features and labels from dataset for local testing\n",
    "        data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "        # Create labels and features\n",
    "        labels, features = targetFeatureSplit(data)\n",
    "\n",
    "        # transform into np.array for StratifiedShuffleSplit\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Shuffle and split data into training/testing sets\n",
    "        sss = StratifiedShuffleSplit()\n",
    "        for train_index, test_index in sss.split(features, labels):\n",
    "            features_train, features_test = features[train_index], features[test_index]\n",
    "            labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        # Create, fit, and predict classifier\n",
    "        clf = clf_choice\n",
    "        clf.fit(features_train, labels_train)\n",
    "        labels_pred = clf.predict(features_test)\n",
    "\n",
    "        try:\n",
    "            precision = precision_score(labels_test, labels_pred)\n",
    "            recall = recall_score(labels_test, labels_pred)\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print \"\\n\" + \"#\" * 60\n",
    "    print \" \" * 20 + name + \"\\n\"\n",
    "    #print confusion_matrix(labels_test, labels_pred)\n",
    "    print \"Precision Mean: \", np.mean(precision_list)\n",
    "    print \"Recall Mean: \", np.mean(recall_list)\n",
    "    #print \"F1 Mean:\", np.nanmean(f1_list)\n",
    "    print \"STD_sum: \", np.std(precision_list) + np.std(recall_list)\n",
    "    print \"\\n\" + \"#\" * 60\n",
    "\n",
    "##### Decision Tree #####\n",
    "# Seems to work best with specfic selected features\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "test_prec_recall(\"Decision Tree\", DecisionTreeClassifier())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state = 49)\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "### Grid Search ###\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#create a dictionary with all the parameters we want to search through\n",
    "param_grid = {'min_samples_split': [2, 5, 10, 15, 20, 25, 30],\n",
    "              'max_depth': [4, 5, 6, 7, 8]}\n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "#create GridSearchCV object using param_grid\n",
    "gridCV_object = GridSearchCV(estimator = DecisionTreeClassifier(random_state = 49),\n",
    "                                         param_grid = param_grid,\n",
    "                                         scoring = 'f1',\n",
    "                                         cv=cross_validator)\n",
    "\n",
    "#fit to the data\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "#what were the best parameters chosen from the parameter grid?\n",
    "print \"Best parameters from parameter grid:\"\n",
    "pprint(gridCV_object.best_params_)\n",
    "\n",
    "#get the best estimator\n",
    "clf_f1 = gridCV_object.best_estimator_\n",
    "\n",
    "#get best parameters for the best estimator.\n",
    "print \"\\nComplete set of parameters for best estimator:\"\n",
    "pprint(clf_f1.get_params())\n",
    "\n",
    "#check scores\n",
    "from sklearn.metrics import f1_score\n",
    "print \"\\nF1 score for default decision tree:\"\n",
    "predictions_1 = clf.predict(features_test)\n",
    "print f1_score(predictions_1, labels_test)\n",
    "\n",
    "print \"\\nF1 score for best estimator from grid search\"\n",
    "predctions_2 = clf_f1.predict(features_test)\n",
    "print f1_score(predctions_2, labels_test)\n",
    "\n",
    "\n",
    "\n",
    "'''##### Random Forest #####\n",
    "# Does okay...\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "test_prec_recall(\"Random Forest\", RandomForestClassifier())\n",
    "\n",
    "##### Extra Trees #####\n",
    "# Current best if using all available features, but still just okay..\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "test_prec_recall(\"Extra Tree\", ExtraTreesClassifier())\n",
    "\n",
    "##### SVMS #####\n",
    "# So far linear, poly, and rfb SVMs are pretty bad at predicting pre-normalize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pipe = Pipeline(steps=[('minmaxscaler', MinMaxScaler()),('clf', SVC())])\n",
    "\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=0.33)\n",
    "\n",
    "pipe.fit(features_train, labels_train)\n",
    "\n",
    "gridCV\n",
    "\n",
    "\n",
    "\n",
    "##### Naive Bayes #####\n",
    "# Naive Bayes never predicts true positive, but can predict true negative.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "test_prec_recall(\"Naive Bayes\", GaussianNB())\n",
    "\n",
    "###### Best clf appears to be Decison Tree;\n",
    "###### precision and recall mean > .3\n",
    "###### generally lowest sum of precision and recall standard deviations\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Parameters\n",
    "\n",
    "Multiple attempts were made to tune the SVM classifier. Two kernals were tested('rbf' and 'poly') along with varying C values from 1 - 1000. Trials to maximize SVM ceased when it appeared the classifier would never outclass either DecisionTree or GaussianNB. \n",
    "\n",
    "The DecisionTree paramenter 'criterion' was test for both 'gini' (default) and 'entropy'. Although close, 'entropy' introduced more deviation (results below) and was not used in the final calculation. \n",
    "~~~\n",
    "############################################################\n",
    "                    Decision Tree (criterion='entropy')\n",
    "\n",
    "Precision Mean:  0.414058344433\n",
    "Recall Mean:  0.383863888889\n",
    "STD_sum:  0.46693555759\n",
    "############################################################\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Validation is usually overcome by splitting the data into training and testing sets. A training size too large can overfit the classifier causing low testing results, whereas a trianing size too low can underfit the classfier, again, causing low testing results.  The Enron Corpus was split into both training (33%) and testing (67%) via function train_test_split with 'test_size = .33'. \n",
    "\n",
    "### Summary\n",
    "\n",
    "The Enron Corpus is one of the largest datasets on fraud. Although the dataset isn't vast, a DecisionTree classifier appears to be a strong option in predicting fraud, followed closely by GaussianNB. Additional/different features for higher precision and recall scores are desired; the current scores appear mediocre but the classifier has potential to be a starting point for detecting fraud.  \n",
    "\n",
    "#### References\n",
    "\n",
    "Feature Visualization:\n",
    "https://public.tableau.com/profile/diego2420#!/vizhome/Udacity/UdacityDashboard\n",
    "\n",
    "Forum Postings:\n",
    "https://discussions.udacity.com/t/getting-started-with-final-project/170846\n",
    "\n",
    "When to chose which machine learning classifier?\n",
    "http://stackoverflow.com/questions/2595176/when-to-choose-which-machine-learning-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
