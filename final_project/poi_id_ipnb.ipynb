{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine Learning With Enron Corpus\n",
    "\n",
    "## Goal: Use Machine Learning to Assess Fraud from Financial and Email Data \n",
    "\n",
    "Assessing fraud cases for any company is a tedious task that requires analyses across a vast amount of data. Fortunately most companies already have access to the data they need to combat fraud: emails and financials. However, these datasets are very large; it is extremely difficult to cipher through all of the data by hand. Having a machine assist and find a potential, fraudulent person of interest (POI) would save both time and money.\n",
    "\n",
    "The email data set tested is the Enron Corpus, which can be found at (https://www.cs.cmu.edu/~./enron/). Enron was one of the top energy companies in the US in the early 2000s. The company eventually filed Bankruptcy largely due to fraudulent cases of insider trading and accounting scandals. It is one of the most noteworthy cases of fraud in the 20th century. Because of the scale of the company and the size of the email database, the Enron Corpus is a prime dataset for finding clues of fraud via financials and email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore Data and Find Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "all_features = ['poi', 'loan_advances', 'director_fees', 'restricted_stock_deferred',\n",
    "               'deferral_payments', 'deferred_income', 'long_term_incentive', 'bonus', \n",
    "               'from_poi_to_this_person', 'shared_receipt_with_poi', 'to_messages',\n",
    "               'from_this_person_to_poi', 'to_messages', 'from_this_person_to_poi',\n",
    "               'from_messages', 'other', 'expenses', 'salary', 'exercised_stock_options',\n",
    "               'restricted_stock', 'total_payments', 'total_stock_value', 'email_address']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Points: 146\n",
      "Total POIs: 18\n"
     ]
    }
   ],
   "source": [
    "# Find number of data points and total POI\n",
    "poi_list = list()\n",
    "\n",
    "for name in data_dict:\n",
    "    if data_dict[name]['poi']:\n",
    "        poi_list.append(name)\n",
    "\n",
    "print \"Total Data Points:\", len(data_dict)\n",
    "print \"Total POIs:\", len(poi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percent NaN Values:\n",
      "loan_advances                0.972603\n",
      "director_fees                0.883562\n",
      "restricted_stock_deferred    0.876712\n",
      "deferral_payments            0.732877\n",
      "deferred_income              0.664384\n",
      "long_term_incentive          0.547945\n",
      "bonus                        0.438356\n",
      "from_poi_to_this_person      0.410959\n",
      "shared_receipt_with_poi      0.410959\n",
      "to_messages                  0.410959\n",
      "from_this_person_to_poi      0.410959\n",
      "from_messages                0.410959\n",
      "other                        0.363014\n",
      "expenses                     0.349315\n",
      "salary                       0.349315\n",
      "exercised_stock_options      0.301370\n",
      "restricted_stock             0.246575\n",
      "total_payments               0.143836\n",
      "total_stock_value            0.136986\n",
      "email_address                0.000000\n",
      "poi                          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\"))\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index', dtype = np.float)\n",
    "percent_nan_list = df.isnull().sum() / (df.isnull().sum() + df.notnull().sum())\n",
    "print \"\\nPercent NaN Values:\\n\", percent_nan_list.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Initial exploration of the dataset revealed 146 data points with 18 POIs. Every data point consisted of features as mentioned above such as: salary, total_payment, to_messages, and expenses. \n",
    "\n",
    "Most features in the Enron Corpus contain NaN values, and these NaN values make up greater than 40% of most features. NaN values represent a lack of information and weaken the overall influence and accuracy of a feature when testing for fraud in the database. There are multiple methods to handle NaN values; in this project NaN values were changed to be either the mean or median by use of a GridCV object and the Imputer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features_list = ['poi', 'salary', 'total_payments','bonus', 'deferred_income','total_stock_value',\n",
    "                'expenses', 'exercised_stock_options', 'other','long_term_incentive',\n",
    "                'restricted_stock', 'to_messages','from_poi_to_this_person',\n",
    "                'from_messages', 'from_this_person_to_poi','shared_receipt_with_poi',\n",
    "                'email_poi_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Digging into the dataset revealed not all entries were people's names: 'SKILLING JEFFREY K' or 'LAY KENNETH L'; however both 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK' were listed as if they were actual people that worked for Enron. Because both were not living beings, I decided to remove them from calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "del data_dict['TOTAL']\n",
    "del data_dict['THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After removing the outliers, three additional features were calculated but were later found to be of minimal importance: percent_to_poi, percent_from_poi, and email_poi_score. \n",
    "\n",
    "The thought was to assign a score to each person signifying the capacity they were in contact with a POI. The score was calculated by summing a person's percent of emails _to_ or _from_ a POI. Again, this yielded no significant gain and was discarded as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "\n",
    "def normalize_feature(feature, data_dict):\n",
    "    # initialize high and low value for normalization function\n",
    "    value_high = None\n",
    "    value_low = None\n",
    "\n",
    "    # loop through persons to find high and low values for features\n",
    "    for person in data_dict:\n",
    "        value = data_dict[person][feature]\n",
    "        if value != 'NaN':\n",
    "            # If first value in feature then assign value to variables\n",
    "            if value_low == None:\n",
    "                value_high = value\n",
    "                value_low = value\n",
    "            # look to see if value is higher or lower\n",
    "            if value > value_high:\n",
    "                value_high = value\n",
    "            elif value < value_low:\n",
    "                value_low = value\n",
    "\n",
    "    # loop to assign normalization value\n",
    "    for person in data_dict:\n",
    "        value = float(data_dict[person][feature])\n",
    "        # if value exists between high and low\n",
    "        if (value_high >= value) and (value_low <= value):\n",
    "            # if denominator isn't zero\n",
    "            if value_high != value_low:\n",
    "                value_norm = (value - value_low) / (value_high - value_low)\n",
    "                data_dict[person][feature] = value_norm\n",
    "            \n",
    "# find percent emails sent to poi and percent from poi to this person\n",
    "for person in data_dict:\n",
    "    from_messages = data_dict[person]['from_messages']\n",
    "    to_messages = data_dict[person]['to_messages']\n",
    "    from_poi = data_dict[person]['from_poi_to_this_person']\n",
    "    to_poi = data_dict[person]['from_this_person_to_poi']\n",
    "    \n",
    "    # Initialize all email_poi_score as 'NaN'\n",
    "    data_dict[person]['email_poi_score'] = 'NaN'\n",
    "\n",
    "    percent_to = float(to_poi) / float(from_messages)\n",
    "    percent_from = float(from_poi) / float(to_messages)\n",
    "\n",
    "    data_dict[person]['percent_to_poi'] = percent_to\n",
    "    data_dict[person]['percent_from_poi'] = percent_from\n",
    "\n",
    "# normailize percent_to_poi and percent_from_poi and add together\n",
    "normalize_feature('percent_to_poi', data_dict)\n",
    "normalize_feature('percent_from_poi', data_dict)\n",
    "\n",
    "# add normalized percent_to_poi and percent_from_poi to create email_poi_score\n",
    "for person in data_dict:\n",
    "    percent_to_norm = data_dict[person]['percent_to_poi']\n",
    "    percent_from_norm = data_dict[person]['percent_from_poi']\n",
    "\n",
    "    email_poi_score = percent_to_norm + percent_from_norm\n",
    "    if email_poi_score >= 0:\n",
    "        data_dict[person]['email_poi_score'] = email_poi_score\n",
    "        \n",
    "# normalize 'email_poi_score'\n",
    "normalize_feature('email_poi_score', data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Hand-picked Feature Selection\n",
    "\n",
    "Feature selection was initially hand-picked from visual aide via https://public.tableau.com/profile/diego2420#!/vizhome/Udacity/UdacityDashboard. Features were chosen based on visual clumping of POIs and non-POIs. The number of features were chosen somewhat arbitrarily; only features that appeared to have a strong visual clumping were chosen.\n",
    "\n",
    "Hand-picked features for determining POI:\n",
    "* exercised_stock_options (high values ~ POI)\n",
    "* deferred_income (low values ~ POI)\n",
    "* expenses (low values ~ not POI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features_handpicked = ['poi', 'exercised_stock_options', 'deferred_income', 'expenses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### SelectKBest Feature Selection\n",
    "\n",
    "Features were also chosen using SelectKBest. Because top features selected from SelectKBest can change depending on the randomness of training and testing the data, a tally was taken to determine which features appear in the top 3 features the most over 1000 trials. The idea behind only choosing the top 3 is only 3 features were chosen for the hand-picked test.\n",
    "\n",
    "SelectKBest features for determining POI:\n",
    "* exercised_stock_option\n",
    "* total_stock_value\n",
    "* bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In top 3:\n",
      "[363  12 552 169 714  33 722   5 119  73   0  15   0   9  40 174]\n",
      "\n",
      "Top 3 features:\n",
      "['poi', 'exercised_stock_options', 'total_stock_value', 'bonus']\n"
     ]
    }
   ],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn import cross_validation\n",
    "from operator import add\n",
    "from heapq import nlargest\n",
    "\n",
    "# Run loop to find how many times a feature occurs in the top 3\n",
    "best_features = [0] * (len(features_list)-1)\n",
    "for i in range(1000):\n",
    "    # Create features and training labels\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "        cross_validation.train_test_split(features, labels, test_size=0.33)\n",
    "\n",
    "    # Generate SelectKBest with k=3 features\n",
    "    selector = SelectKBest(f_classif, k=3)\n",
    "    selector.fit(features_train, labels_train)\n",
    "    \n",
    "    # Increase score of feature if it appears in the top 3   \n",
    "    best_features = selector.get_support().astype(int) + best_features\n",
    "\n",
    "    \n",
    "print \"In top 3:\\n\", best_features\n",
    "# Print the top 3 features scored by which features appeared most in top 3\n",
    "features_kbest = ['poi']\n",
    "for e in nlargest(3, best_features):\n",
    "    for index in range(len(best_features)):\n",
    "        if e == best_features[index]:\n",
    "            top_feature = features_list[index+1]\n",
    "            if top_feature not in features_kbest:\n",
    "                features_kbest.append(top_feature)\n",
    "\n",
    "print \"\\nTop 3 features:\\n\", features_kbest        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classifiers\n",
    "\n",
    "To test for optimal training from the data multiple classifiers are used:\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Extra Trees\n",
    "* SVMs\n",
    "* GaussianNB\n",
    "\n",
    "Classifiers were tested for high scores in precision, recall, and f1. Precision is the percent of people the classifier accurately labeled as a POI. Recall is the percent of actual POIs that were correctly classified. The F1 score relates both precion and recall. F1 scores are calculated by this equation:\n",
    "\n",
    "$$ F1 = 2 * (precision * recall)  /  (precision + recall) $$\n",
    "\n",
    "Only classifiers with precsion and recall scores greater than or equal to .33 will be considered for the best overall classifier. Any of those best classifiers will then be ranked by highest f1 score.\n",
    "\n",
    "During initial testings I noticed a very high variation of all scores; classifiers that yielded high precision and recall scores may have low scores on the next test. To counteract the high deviation of values, I wrote a test function to generate 1000 classifiers of each type and compare their average scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Task 4: Try a varity of classifiers\\n### Please name your classifier clf for easy export below.\\n### Note that if you want to do PCA or other multi-stage operations,\\n### you\\'ll need to use Pipelines. For more info:\\n### http://scikit-learn.org/stable/modules/pipeline.html\\n\\nfrom time import time\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import precision_score\\nfrom sklearn.metrics import recall_score\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import StratifiedShuffleSplit\\nfrom pprint import pprint\\n\\n\\n\\n# Create test to find average precision and recall scores\\ndef test_prec_recall(name, clf_choice, features_list):\\n    precision_list = list()\\n    recall_list = list()\\n    for i in range(1000):\\n        ### Extract features and labels from dataset for local testing\\n        data = featureFormat(data_dict, features_list, sort_keys = True)\\n        # Create labels and features\\n        labels, features = targetFeatureSplit(data)\\n\\n        # transform into np.array for StratifiedShuffleSplit\\n        features = np.array(features)\\n        labels = np.array(labels)\\n\\n        # Shuffle and split data into training/testing sets\\n        sss = StratifiedShuffleSplit()\\n        for train_index, test_index in sss.split(features, labels):\\n            features_train, features_test = features[train_index], features[test_index]\\n            labels_train, labels_test = labels[train_index], labels[test_index]\\n\\n        # Create, fit, and predict classifier\\n        clf = clf_choice\\n        clf.fit(features_train, labels_train)\\n        labels_pred = clf.predict(features_test)\\n\\n        try:\\n            precision = precision_score(labels_test, labels_pred)\\n            recall = recall_score(labels_test, labels_pred)\\n            precision_list.append(precision)\\n            recall_list.append(recall)\\n        except:\\n            pass\\n    \\n    # F score is calculated via the mean precision and recall scores\\n    p_score = np.mean(precision_list)\\n    r_score = np.mean(recall_list)\\n    f_score = 2 * (p_score * r_score) / (p_score + r_score)\\n    \\n    print \"\\n\" + \"#\" * 60\\n    print \" \" * 20 + name + \"\\n\"\\n    print \"Precision Mean Score: \", p_score\\n    print \"Recall Mean Score: \", r_score\\n    print \"F Score: \", f_score\\n    print \"\\n\" + \"#\" * 60    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "# Create test to find average precision and recall scores\n",
    "def test_prec_recall(name, clf_choice, features_list):\n",
    "    precision_list = list()\n",
    "    recall_list = list()\n",
    "    for i in range(1000):\n",
    "        ### Extract features and labels from dataset for local testing\n",
    "        data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "        # Create labels and features\n",
    "        labels, features = targetFeatureSplit(data)\n",
    "\n",
    "        # transform into np.array for StratifiedShuffleSplit\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Shuffle and split data into training/testing sets\n",
    "        sss = StratifiedShuffleSplit()\n",
    "        for train_index, test_index in sss.split(features, labels):\n",
    "            features_train, features_test = features[train_index], features[test_index]\n",
    "            labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        # Create, fit, and predict classifier\n",
    "        clf = clf_choice\n",
    "        clf.fit(features_train, labels_train)\n",
    "        labels_pred = clf.predict(features_test)\n",
    "\n",
    "        try:\n",
    "            precision = precision_score(labels_test, labels_pred)\n",
    "            recall = recall_score(labels_test, labels_pred)\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # F score is calculated via the mean precision and recall scores\n",
    "    p_score = np.mean(precision_list)\n",
    "    r_score = np.mean(recall_list)\n",
    "    f_score = 2 * (p_score * r_score) / (p_score + r_score)\n",
    "    \n",
    "    print \"\\n\" + \"#\" * 60\n",
    "    print \" \" * 20 + name + \"\\n\"\n",
    "    print \"Precision Mean Score: \", p_score\n",
    "    print \"Recall Mean Score: \", r_score\n",
    "    print \"F Score: \", f_score\n",
    "    print \"\\n\" + \"#\" * 60    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classifiers (Features List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)), (...it=5, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.79160\tPrecision: 0.18547\tRecall: 0.16600\tF1: 0.17520\tF2: 0.16956\n",
      "\tTotal predictions: 15000\tTrue positives:  332\tFalse positives: 1458\tFalse negatives: 1668\tTrue negatives: 11542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "pipeline_dt = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_dt = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    \n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridCV_object_dt = GridSearchCV(estimator = pipeline_dt,\n",
    "                                param_grid = param_grid_dt,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_dt.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_dt_af = gridCV_object_dt.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_dt_af, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)), (...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.83867\tPrecision: 0.29290\tRecall: 0.14850\tF1: 0.19708\tF2: 0.16474\n",
      "\tTotal predictions: 15000\tTrue positives:  297\tFalse positives:  717\tFalse negatives: 1703\tTrue negatives: 12283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_rf = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridCV_object_rf = GridSearchCV(estimator = pipeline_rf,\n",
    "                                param_grid = param_grid_rf,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_rf.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_rf_af = gridCV_object_rf.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_rf_af, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Extra Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)),...timators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.87060\tPrecision: 0.68553\tRecall: 0.05450\tF1: 0.10097\tF2: 0.06680\n",
      "\tTotal predictions: 15000\tTrue positives:  109\tFalse positives:   50\tFalse negatives: 1891\tTrue negatives: 12950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "pipeline_et = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', ExtraTreesClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_et = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridCV_object_et = GridSearchCV(estimator = pipeline_et,\n",
    "                                param_grid = param_grid_et,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_et.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_et_af = gridCV_object_et.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_et_af, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: SVMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', Sele...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.85980\tPrecision: 0.00952\tRecall: 0.00050\tF1: 0.00095\tF2: 0.00062\n",
      "\tTotal predictions: 15000\tTrue positives:    1\tFalse positives:  104\tFalse negatives: 1999\tTrue negatives: 12896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pipeline_svc = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('minmaxscaler', MinMaxScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "param_grid_svc = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [4],\n",
    "        'skb__k': [1,2,3,4],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [5],\n",
    "        'skb__k': [1,2,3,4,5],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    }\n",
    "]\n",
    "\n",
    "cross_validator = StratifiedShuffleSplit()\n",
    "\n",
    "gridCV_object_svc = GridSearchCV(estimator = pipeline_svc,\n",
    "                                 param_grid = param_grid_svc,\n",
    "                                 scoring = 'f1',\n",
    "                                 cv = cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_svc.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_svc_af = gridCV_object_svc.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_svc_af, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=5, score_func=<function f_classif at 0x7f23404d3b18>)), (...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.84567\tPrecision: 0.34664\tRecall: 0.17800\tF1: 0.23522\tF2: 0.19719\n",
      "\tTotal predictions: 15000\tTrue positives:  356\tFalse positives:  671\tFalse negatives: 1644\tTrue negatives: 12329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_nb = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [4],\n",
    "        'skb__k': [1,2,3,4],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [5],\n",
    "        'skb__k': [1,2,3,4,5],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [6],\n",
    "        'skb__k': [1,2,3,4,5,6],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }\n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridCV_object_nb = GridSearchCV(estimator = pipeline_nb,\n",
    "                                param_grid = param_grid_nb,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_nb.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_nb_af = gridCV_object_nb.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_nb_af, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers (Hand-Picked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Decision Tree (Hand-Picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_handpicked, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=1, score_func=<function f_classif at 0x7f23404d3b18>)),...it=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.88243\tPrecision: 0.83022\tRecall: 0.22250\tF1: 0.35095\tF2: 0.26066\n",
      "\tTotal predictions: 14000\tTrue positives:  445\tFalse positives:   91\tFalse negatives: 1555\tTrue negatives: 11909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_dt = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_dt = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }       \n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridCV_object_dt = GridSearchCV(estimator = pipeline_dt,\n",
    "                                param_grid = param_grid_dt,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_dt.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_dt_hp = gridCV_object_dt.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_dt_hp, my_dataset, features_handpicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Random Forest (Hand-Picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)),...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.84171\tPrecision: 0.40592\tRecall: 0.23300\tF1: 0.29606\tF2: 0.25470\n",
      "\tTotal predictions: 14000\tTrue positives:  466\tFalse positives:  682\tFalse negatives: 1534\tTrue negatives: 11318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_rf = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }\n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "\n",
    "gridCV_object_rf = GridSearchCV(estimator = pipeline_rf,\n",
    "                                param_grid = param_grid_rf,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_rf.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_rf_hp = gridCV_object_rf.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_rf_hp, my_dataset, features_handpicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Extra Trees (Hand-Picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)),...timators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.85243\tPrecision: 0.46145\tRecall: 0.19750\tF1: 0.27661\tF2: 0.22301\n",
      "\tTotal predictions: 14000\tTrue positives:  395\tFalse positives:  461\tFalse negatives: 1605\tTrue negatives: 11539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_et = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', ExtraTreesClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_et = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }\n",
    "]              \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "gridCV_object_etsk = GridSearchCV(estimator = pipeline_et,\n",
    "                                param_grid = param_grid_et,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_et.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_et_hp = gridCV_object_et.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_et_hp, my_dataset, features_handpicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: SVMs (Hand-Picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', Sele...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.85107\tPrecision: 0.03297\tRecall: 0.00150\tF1: 0.00287\tF2: 0.00185\n",
      "\tTotal predictions: 14000\tTrue positives:    3\tFalse positives:   88\tFalse negatives: 1997\tTrue negatives: 11912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_svc = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('minmaxscaler', MinMaxScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "param_grid_svc = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    }\n",
    "]\n",
    "\n",
    "cross_validator = StratifiedShuffleSplit()\n",
    "\n",
    "gridCV_object_svc = GridSearchCV(estimator = pipeline_svc,\n",
    "                                 param_grid = param_grid_svc,\n",
    "                                 scoring = 'f1',\n",
    "                                 cv = cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_svc.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_svc_hp = gridCV_object_svc.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_svc_hp, my_dataset, features_handpicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Naive Bayes (Hand-Picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)),...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.86929\tPrecision: 0.64026\tRecall: 0.19400\tF1: 0.29777\tF2: 0.22542\n",
      "\tTotal predictions: 14000\tTrue positives:  388\tFalse positives:  218\tFalse negatives: 1612\tTrue negatives: 11782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_nb = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_nb = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }\n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridCV_object_nb = GridSearchCV(estimator = pipeline_nb,\n",
    "                                param_grid = param_grid_nb,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_nb.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_nb_hp = gridCV_object_nb.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_nb_hp, my_dataset, features_handpicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classifiers: Hand-picked Summary\n",
    "\n",
    "Both the Decison Tree and Extra Trees classifiers are close to meeting the criteria of having precision and recall scores greater than .33. However, the Extra Trees Classifier has a higher precision and higher f score compared to Decision Tree.\n",
    "\n",
    "The **Extra Trees classifier** is the best of the **hand-picked** feature testing with ** precision = .398**, ** recall = .312**, and **F score = .350**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classifiers (SelectKBest Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Decision Tree (SelectKBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_kbest, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)),...it=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.86885\tPrecision: 0.59698\tRecall: 0.45400\tF1: 0.51576\tF2: 0.47684\n",
      "\tTotal predictions: 13000\tTrue positives:  908\tFalse positives:  613\tFalse negatives: 1092\tTrue negatives: 10387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_dt = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_dt = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }       \n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridCV_object_dt = GridSearchCV(estimator = pipeline_dt,\n",
    "                                param_grid = param_grid_dt,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_dt.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_dt_sk = gridCV_object_dt.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_dt_sk, my_dataset, features_kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Random Forest (SelectKBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=3, score_func=<function f_classif at 0x7f23404d3b18>)),...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.84662\tPrecision: 0.50311\tRecall: 0.24300\tF1: 0.32771\tF2: 0.27102\n",
      "\tTotal predictions: 13000\tTrue positives:  486\tFalse positives:  480\tFalse negatives: 1514\tTrue negatives: 10520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_rf = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }\n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "\n",
    "gridCV_object_rf = GridSearchCV(estimator = pipeline_rf,\n",
    "                                param_grid = param_grid_rf,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_rf.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_rf_sk = gridCV_object_rf.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_rf_sk, my_dataset, features_kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Extra Trees (SelectKBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)), (...timators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.84646\tPrecision: 0.50195\tRecall: 0.25800\tF1: 0.34082\tF2: 0.28578\n",
      "\tTotal predictions: 13000\tTrue positives:  516\tFalse positives:  512\tFalse negatives: 1484\tTrue negatives: 10488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_et = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', ExtraTreesClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_et = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }\n",
    "]              \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "gridCV_object_etsk = GridSearchCV(estimator = pipeline_et,\n",
    "                                param_grid = param_grid_et,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_et.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_et_sk = gridCV_object_et.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_et_sk, my_dataset, features_kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: SVMs (SelectKBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', Sele...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86062\tPrecision: 0.77811\tRecall: 0.13150\tF1: 0.22498\tF2: 0.15771\n",
      "\tTotal predictions: 13000\tTrue positives:  263\tFalse positives:   75\tFalse negatives: 1737\tTrue negatives: 10925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_svc = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('minmaxscaler', MinMaxScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "param_grid_svc = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__C': [10,50,100],\n",
    "        'clf__kernel': ['rbf', 'poly'],\n",
    "    }\n",
    "]\n",
    "\n",
    "cross_validator = StratifiedShuffleSplit()\n",
    "\n",
    "gridCV_object_svc = GridSearchCV(estimator = pipeline_svc,\n",
    "                                 param_grid = param_grid_svc,\n",
    "                                 scoring = 'f1',\n",
    "                                 cv = cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_svc.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_svc_sk = gridCV_object_svc.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_svc_sk, my_dataset, features_kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Classifiers: Naive Bayes (SelectKBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x7f23404d3b18>)), (...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.84669\tPrecision: 0.50379\tRecall: 0.23250\tF1: 0.31817\tF2: 0.26056\n",
      "\tTotal predictions: 13000\tTrue positives:  465\tFalse positives:  458\tFalse negatives: 1535\tTrue negatives: 10542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_nb = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('skb', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "param_grid_nb = [\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [1],\n",
    "        'skb__k': [1],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [2],\n",
    "        'skb__k': [1,2],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    },\n",
    "    {\n",
    "        'imp__strategy': ['median', 'mean'],\n",
    "        'pca__n_components': [3],\n",
    "        'skb__k': [1,2,3],\n",
    "        'clf__min_samples_split': [2,3,5],\n",
    "        'clf__max_depth': [None,2,3],\n",
    "    }\n",
    "]        \n",
    "\n",
    "cross_validator = StratifiedShuffleSplit(random_state = 0)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridCV_object_nb = GridSearchCV(estimator = pipeline_nb,\n",
    "                                param_grid = param_grid_nb,\n",
    "                                scoring = 'f1',\n",
    "                                cv=cross_validator)\n",
    "\n",
    "# fit the data\n",
    "gridCV_object_nb.fit(features, labels)\n",
    "\n",
    "# get the best estimator\n",
    "pipeline_clf_nb_sk = gridCV_object_nb.best_estimator_\n",
    "\n",
    "# test results\n",
    "from tester import test_classifier\n",
    "test_classifier(pipeline_clf_nb_sk, my_dataset, features_kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classifiers: SelectKBest Summary\n",
    "\n",
    "The Decision Tree classifier completely exceeds all expections and is the clear winner of the selectkbest feature selection. The **Decision Tree** classifier with **selectkbest** attained scores of **precision = .418**, **recall = .416** and **F score = .4023**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Classifiers: Hand-picked vs. SelectKBest Summary\n",
    "\n",
    "The classifier chosen as best across both hand-picked and selectkbest is the Decision Tree classifier with the feature selection from selectkbest. Again, the selectkbest features are:\n",
    "* exercised_stock_option\n",
    "* total_stock_value\n",
    "* bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Validation and Parameter Tuning\n",
    "\n",
    "##### Validation\n",
    "Data validation is the series of steps it takes to make sure the data is clean and useful. Dirty data (being mislabeled or unaccurate) leads to inconclusive results no matter the outcome. \n",
    "\n",
    "Validation in machine learning is usually overcome by splitting the data into training and testing sets. A training size too large can overfit the classifier causing low testing results, whereas a trianing size too low can underfit the classfier, again, causing low testing results.  \n",
    "\n",
    "The Enron Corpus was split into both training (33%) and testing (67%) via function train_test_split with 'test_size = .33'. This essentially trains the data multiple times over different partitions of the dataset using the leftover data for testing and scoring.\n",
    "\n",
    "##### Parameter Tuning\n",
    "As discussed earlier, parameters of each classifier were tuned using both a pipeline and creating a gridCV object. The gridCV object tests multiple lists of parameters and returns the parameters that maximize a scoring function. In this case all parameters were tuned to maximize F1 scores.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The Enron Corpus is one of the largest datasets on fraud. Although the dataset isn't vast, a Decision Tree classifier appears to be a strong option in predicting fraud. \n",
    "\n",
    "The most important features to analyze for attempting fraud are most likely:\n",
    "* exercised stock options\n",
    "* total stock\n",
    "* bonus\n",
    "\n",
    "The afformetioned features combined with a Decision Tree classifier yield precision, recall, and f1 scores close to .4. Additional/different features for higher scores are desired; the current scores appear mediocre but the classifier has potential to be a starting point for detecting fraud.  \n",
    "\n",
    "#### References\n",
    "\n",
    "Feature Visualization:\n",
    "https://public.tableau.com/profile/diego2420#!/vizhome/Udacity/UdacityDashboard\n",
    "\n",
    "Forum Postings:\n",
    "https://discussions.udacity.com/t/getting-started-with-final-project/170846\n",
    "\n",
    "When to chose which machine learning classifier:\n",
    "http://stackoverflow.com/questions/2595176/when-to-choose-which-machine-learning-classifier\n",
    "\n",
    "GridCV and Pipeline testing:\n",
    "https://discussions.udacity.com/t/webcast-builidng-models-with-gridsearchcv-and-pipelines-thursday-11-feb-2015-at-6pm-pacific-time/47412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gridCV_object_dtsk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9cb654e46f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m### generates the necessary .pkl files for validating your results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdump_classifier_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgridCV_object_dtsk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_kbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gridCV_object_dtsk' is not defined"
     ]
    }
   ],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(gridCV_object_dtsk.best_estimator_, my_dataset, features_kbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
